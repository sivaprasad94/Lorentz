{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating L.E for Mackey Glass data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import ESN\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set seed for random weights generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed used for random values: 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def set_seed(seed=None):\n",
    "    \"\"\"Making the seed (for random values) variable if None\"\"\"\n",
    "\n",
    "    # Set the seed\n",
    "    if seed is None:\n",
    "        import time\n",
    "        seed = int((time.time()*10**6) % 4294967295)\n",
    "    try:\n",
    "        np.random.seed(seed)\n",
    "    except Exception as e:\n",
    "        print( \"!!! WARNING !!!: Seed was not set correctly.\")\n",
    "        print( \"!!! Seed that we tried to use: \"+str(seed))\n",
    "        print( \"!!! Error message: \"+str(e))\n",
    "        seed = None\n",
    "    print( \"Seed used for random values:\", seed)\n",
    "    return seed\n",
    "## Set a particular seed for the random generator (for example seed = 42), or use a \"random\" one (seed = None)\n",
    "# NB: reservoir performances should be averaged accross at least 30 random instances (with the same set of parameters)\n",
    "seed = 42 #None #42\n",
    "set_seed(seed) #random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting with Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(1,101):\n",
    "    df =  pd.read_excel(r'C:\\Users\\INFO-DSK-02\\Desktop\\Lorentz Multi Dimension Prediction-Phase-2\\Final_Version\\3D_ReservoirComputing\\Input\\Mackey Glass Data\\MCglass.xlsx', index = False)\n",
    "    initLen = 1000\n",
    "    trainLen = initLen + 500\n",
    "    testLen = 350\n",
    "    df['x'][1500] = df['x'][1500] + 1e-7\n",
    "    data_in = df[['x']]\n",
    "    data_T  =df['t']\n",
    "    data_in = np.array(data_in)\n",
    "    data_t = np.array(data_T)\n",
    "    train_in = np.array(data_in[0:trainLen])\n",
    "    train_out = np.array(data_in[0+1:trainLen+1])\n",
    "    test_in = np.array(data_in[trainLen:trainLen+testLen])\n",
    "    test_out = np.array(data_in[trainLen+1:trainLen+testLen+1])\n",
    "    train_in_t = np.array(data_T[0:trainLen])\n",
    "    train_out_t = np.array(data_T[0+1:trainLen])\n",
    "    test_in_t = np.array(data_T[trainLen:trainLen+testLen])\n",
    "    test_out_t = np.array(data_T[trainLen+1:trainLen+testLen+1])\n",
    "    n_reservoir = 1000 # number of recurrent units\n",
    "    leak_rate = 0.3 # leaking rate (=1/time_constant_of_neurons)\n",
    "    spectral_radius = 0.8 # Scaling of recurrent matrix\n",
    "    input_scaling = 1. # Scaling of input matrix\n",
    "    proba_non_zero_connec_W = 0.2 # Sparsity of recurrent matrix: Perceptage of non-zero connections in W matrix\n",
    "    proba_non_zero_connec_Win = 1. # Sparsity of input matrix\n",
    "    proba_non_zero_connec_Wfb = 1. # Sparsity of feedback matrix\n",
    "    regularization_coef =  0.01 #None # regularization coefficient, if None, pseudo-inverse is use instead of ridge regression\n",
    "    n_inputs = 1\n",
    "    input_bias = True # add a constant input to 1\n",
    "    n_outputs = 1\n",
    "    N = n_reservoir#100\n",
    "    dim_inp = n_inputs #26\n",
    "    ### Generating random weight matrices with custom method\n",
    "    W = np.random.rand(N,N) - 0.5\n",
    "    if input_bias:\n",
    "        Win = np.random.rand(N,dim_inp+1) - 0.5\n",
    "    else:\n",
    "        Win = np.random.rand(N,dim_inp) - 0.5\n",
    "    Wfb = np.random.rand(N,n_outputs) - 0.5  \n",
    "    ## delete the fraction of connections given the sparsity (i.e. proba of non-zero connections):\n",
    "    mask = np.random.rand(N,N) # create a mask Uniform[0;1]\n",
    "    W[mask > proba_non_zero_connec_W] = 0 # set to zero some connections given by the mask\n",
    "    mask = np.random.rand(N,Win.shape[1])\n",
    "    Win[mask > proba_non_zero_connec_Win] = 0\n",
    "    # mask = np.random.rand(N,Wfb.shape[1])\n",
    "    # Wfb[mask > proba_non_zero_connec_Wfb] = 0\n",
    "    ## SCALING of matrices\n",
    "    # scaling of input matrix\n",
    "    Win = Win * input_scaling\n",
    "    # scaling of recurrent matrix\n",
    "    # compute the spectral radius of these weights:\n",
    "#     print( 'Computing spectral radius...')\n",
    "    original_spectral_radius = np.max(np.abs(np.linalg.eigvals(W)))\n",
    "    #TODO: check if this operation is quicker: max(abs(linalg.eig(W)[0])) #from scipy import linalg\n",
    "#     print( \"default spectral radius before scaling:\", original_spectral_radius)\n",
    "    # rescale them to reach the requested spectral radius:\n",
    "    W = W * (spectral_radius / original_spectral_radius)\n",
    "#     print( \"spectral radius after scaling\", np.max(np.abs(np.linalg.eigvals(W))))\n",
    "    reservoir = ESN.ESN(lr=leak_rate, W=W, Win=Win, input_bias=input_bias, ridge=regularization_coef, Wfb=None, fbfunc=None)\n",
    "    internal_trained = reservoir.train(inputs=[train_in], teachers=[train_out], wash_nr_time_step=initLen, verbose=False)\n",
    "    output_pred, internal_pred = reservoir.run(inputs=[test_in,], reset_state=False)\n",
    "    df_pred = pd.DataFrame(output_pred[0])\n",
    "    errorLen = len(test_out[:]) #testLen #2000\n",
    "    test_out = pd.DataFrame(test_out)\n",
    "    df_pred.columns= ['X_pred']\n",
    "    test_out.columns = ['X_test']\n",
    "    df_out = pd.concat([df_pred, test_out], axis = 1)\n",
    "    df_out['Test_T'] = test_out_t\n",
    "    df_out.to_excel(r'C:\\Users\\INFO-DSK-02\\Desktop\\Lorentz Multi Dimension Prediction-Phase-2\\Final_Version\\3D_ReservoirComputing\\Output\\MC_Data\\100_Trails\\With_Noise\\Train_{}_with_noise_pred.xlsx'.format(i), index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting without noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,101):\n",
    "    df =  pd.read_excel(r'C:\\Users\\INFO-DSK-02\\Desktop\\Lorentz Multi Dimension Prediction-Phase-2\\Final_Version\\3D_ReservoirComputing\\Input\\Mackey Glass Data\\MCglass.xlsx', index = False)\n",
    "    initLen = 1000\n",
    "    trainLen = initLen + 500\n",
    "    testLen = 350\n",
    "#     df['x'][0:1000] = df['x'][0:1000] + 1e-7\n",
    "    data_in = df[['x']]\n",
    "    data_T  =df['t']\n",
    "    data_in = np.array(data_in)\n",
    "    data_t = np.array(data_T)\n",
    "    train_in = np.array(data_in[0:trainLen])\n",
    "    train_out = np.array(data_in[0+1:trainLen+1])\n",
    "    test_in = np.array(data_in[trainLen:trainLen+testLen])\n",
    "    test_out = np.array(data_in[trainLen+1:trainLen+testLen+1])\n",
    "    train_in_t = np.array(data_T[0:trainLen])\n",
    "    train_out_t = np.array(data_T[0+1:trainLen])\n",
    "    test_in_t = np.array(data_T[trainLen:trainLen+testLen])\n",
    "    test_out_t = np.array(data_T[trainLen+1:trainLen+testLen+1])\n",
    "    n_reservoir = 1000 # number of recurrent units\n",
    "    leak_rate = 0.3 # leaking rate (=1/time_constant_of_neurons)\n",
    "    spectral_radius = 0.8 # Scaling of recurrent matrix\n",
    "    input_scaling = 1. # Scaling of input matrix\n",
    "    proba_non_zero_connec_W = 0.2 # Sparsity of recurrent matrix: Perceptage of non-zero connections in W matrix\n",
    "    proba_non_zero_connec_Win = 1. # Sparsity of input matrix\n",
    "    proba_non_zero_connec_Wfb = 1. # Sparsity of feedback matrix\n",
    "    regularization_coef =  0.01 #None # regularization coefficient, if None, pseudo-inverse is use instead of ridge regression\n",
    "    n_inputs = 1\n",
    "    input_bias = True # add a constant input to 1\n",
    "    n_outputs = 1\n",
    "    N = n_reservoir#100\n",
    "    dim_inp = n_inputs #26\n",
    "#     df['x'][0:1000] = df['x'][0:1000] + 1e-7\n",
    "    ### Generating random weight matrices with custom method\n",
    "    W = np.random.rand(N,N) - 0.5\n",
    "    if input_bias:\n",
    "        Win = np.random.rand(N,dim_inp+1) - 0.5\n",
    "    else:\n",
    "        Win = np.random.rand(N,dim_inp) - 0.5\n",
    "    Wfb = np.random.rand(N,n_outputs) - 0.5  \n",
    "    ## delete the fraction of connections given the sparsity (i.e. proba of non-zero connections):\n",
    "    mask = np.random.rand(N,N) # create a mask Uniform[0;1]\n",
    "    W[mask > proba_non_zero_connec_W] = 0 # set to zero some connections given by the mask\n",
    "    mask = np.random.rand(N,Win.shape[1])\n",
    "    Win[mask > proba_non_zero_connec_Win] = 0\n",
    "    # mask = np.random.rand(N,Wfb.shape[1])\n",
    "    # Wfb[mask > proba_non_zero_connec_Wfb] = 0\n",
    "    ## SCALING of matrices\n",
    "    # scaling of input matrix\n",
    "    Win = Win * input_scaling\n",
    "    # scaling of recurrent matrix\n",
    "    # compute the spectral radius of these weights:\n",
    "#     print( 'Computing spectral radius...')\n",
    "    original_spectral_radius = np.max(np.abs(np.linalg.eigvals(W)))\n",
    "    #TODO: check if this operation is quicker: max(abs(linalg.eig(W)[0])) #from scipy import linalg\n",
    "#     print( \"default spectral radius before scaling:\", original_spectral_radius)\n",
    "    # rescale them to reach the requested spectral radius:\n",
    "    W = W * (spectral_radius / original_spectral_radius)\n",
    "#     print( \"spectral radius after scaling\", np.max(np.abs(np.linalg.eigvals(W))))\n",
    "    reservoir = ESN.ESN(lr=leak_rate, W=W, Win=Win, input_bias=input_bias, ridge=regularization_coef, Wfb=None, fbfunc=None)\n",
    "    internal_trained = reservoir.train(inputs=[train_in], teachers=[train_out], wash_nr_time_step=initLen, verbose=False)\n",
    "    output_pred, internal_pred = reservoir.run(inputs=[test_in,], reset_state=False)\n",
    "    df_pred = pd.DataFrame(output_pred[0])\n",
    "    errorLen = len(test_out[:]) #testLen #2000\n",
    "    test_out = pd.DataFrame(test_out)\n",
    "    df_pred.columns= ['X_pred']\n",
    "    test_out.columns = ['X_test']\n",
    "    df_out = pd.concat([df_pred, test_out], axis = 1)\n",
    "    df_out['Test_T'] = test_out_t\n",
    "    df_out.to_excel(r'C:\\Users\\INFO-DSK-02\\Desktop\\Lorentz Multi Dimension Prediction-Phase-2\\Final_Version\\3D_ReservoirComputing\\Output\\MC_Data\\100_Trails\\With_Out_Noise\\Train_{}_without_noise_pred.xlsx'.format(i), index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_l_e_s = []\n",
    "for i in range(1,101):\n",
    "    df_noise = pd.read_excel(r'C:\\Users\\INFO-DSK-02\\Desktop\\Lorentz Multi Dimension Prediction-Phase-2\\Final_Version\\3D_ReservoirComputing\\Output\\MC_Data\\100_Trails\\With_Noise\\Train_{}_with_noise_pred.xlsx'.format(i))\n",
    "    df_without_noise = pd.read_excel(r'C:\\Users\\INFO-DSK-02\\Desktop\\Lorentz Multi Dimension Prediction-Phase-2\\Final_Version\\3D_ReservoirComputing\\Output\\MC_Data\\100_Trails\\With_Out_Noise\\Train_{}_without_noise_pred.xlsx'.format(i))\n",
    "    d1 = abs(sum(df_noise[101:118]['X_pred'] - df_without_noise[101:118]['X_pred'])/17)\n",
    "    d2 = abs(sum(df_noise[301:318]['X_pred'] - df_without_noise[301:318]['X_pred'])/17)\n",
    "    l_l_e_s.append(math.log(abs(d2)/abs(d1))/200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L.L.E:  0.005341868100076218\n"
     ]
    }
   ],
   "source": [
    "print('L.L.E: ',  sum([abs(x) for x in l_l_e_s])/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00171167, 0.00256173, 0.00296956, ...,        nan,        nan,\n",
       "              nan])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyentrp import entropy as ent\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "ts = df['x'].values\n",
    "std_ts = np.std(ts)\n",
    "sample_entropy_250 = ent.multiscale_entropy(ts,250)\n",
    "sample_entropy_250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00109432, 0.00141769, 0.00060932, ...,        nan,        nan,\n",
       "              nan])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_entropy_500 = ent.multiscale_entropy(ts,500)\n",
    "sample_entropy_500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00106316, 0.00047921, 0.0004865 , ...,        nan,        nan,\n",
       "              nan])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_entropy_750 = ent.multiscale_entropy(ts,750)\n",
    "sample_entropy_750"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00049902, 0.00056859, 0.00059506, ...,        nan,        nan,\n",
       "              nan])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_entropy_1000 = ent.multiscale_entropy(ts,1000)\n",
    "sample_entropy_1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0., nan, nan, ..., nan, nan, nan])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_entropy_10000 = ent.multiscale_entropy(ts,10000)\n",
    "sample_entropy_10000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
